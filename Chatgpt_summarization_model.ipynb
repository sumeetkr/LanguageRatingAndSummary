{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38c7035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d67994f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2004, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"AAAI 2024/kidsInMindSubtitles2004 (2).csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8577d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e51bdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c0346d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_language_description(subtitle):\n",
    "    # Language model prompt for GPT-3.5\n",
    "    prompt = f\"Given the list of subtitle: '{subtitle}', describe the language used in the video in 2 lines depending on quantity (more F-words, for instance, will mean a higher Language rating, and so on) as well as context (especially when it comes to the categories of sex, nudity, violence and gore, since they are not as easily quantifiable as profanity. Indicate each type of profane words for each of them.\"\n",
    "\n",
    "    # Initialize conversation with the initial prompt\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a language describer.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Make an API call to GPT-3.5\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "#         messages=conversation,\n",
    "        prompt=prompt,\n",
    "        max_tokens=500,  # Adjust this parameter to control the response length\n",
    "        temperature=0.7,  # Adjust this parameter to control the response randomness\n",
    "    )\n",
    "\n",
    "    # Extract the language description from the model's response\n",
    "    language_description = response.choices[0].text.strip()\n",
    "\n",
    "    return language_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c7da163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_subtitle(subtitle, max_tokens):\n",
    "    if len(subtitle) <= max_tokens:\n",
    "        return subtitle\n",
    "    \n",
    "    words = subtitle.split()\n",
    "\n",
    "    shortened_subtitle = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if len(shortened_subtitle) + len(word) + 1 <= max_tokens:\n",
    "            shortened_subtitle += word + \" \"\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    shortened_subtitle = shortened_subtitle.strip()\n",
    "\n",
    "    return shortened_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed51e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "780ffb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['subtitles'].tolist()\n",
    "labels_regression = df['Language'].tolist()\n",
    "predictions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ab667ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for text in texts[2003:]:\n",
    "    shortened_subtitle=shorten_subtitle(text, 2000)\n",
    "    predictions.append(generate_language_description(shortened_subtitle))\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d08947f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1cf89fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_chatgpt=pd.DataFrame({\"Predictions\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7bc389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_chatgpt.to_excel(\"summaries_Chatgpt.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692c6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf2811bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel=pd.read_excel(\"summaries_Chatgpt.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "471755df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4a0b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ground_truth=texts[1803:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc0a826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Language_Description=df[\"Language_Description\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "105be884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c247c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_chatgpt=pd.DataFrame({\"Subtitles\":Ground_truth,\"Language_Description\":Language_Description[1803:],\"Predictions\":df_excel['Predictions']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63518456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_chatgpt.to_excel(\"predictions_summarization_chatgpt.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3862a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "summaries = predictions\n",
    "references = Ground_truth\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for summary, reference in zip(summaries, references):\n",
    "    scores = scorer.score(summary, reference)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "average_rouge1_score = sum(rouge1_scores) / len(rouge1_scores)\n",
    "average_rouge2_score = sum(rouge2_scores) / len(rouge2_scores)\n",
    "average_rougeL_score = sum(rougeL_scores) / len(rougeL_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad24fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_rouge1_score:  0.005959579102255901\n",
      "average_rouge2_score:  0.0009530633869575544\n",
      "average_rougeL_score:  0.005053405809320419\n"
     ]
    }
   ],
   "source": [
    "print(\"average_rouge1_score: \",average_rouge1_score)\n",
    "print(\"average_rouge2_score: \",average_rouge2_score)\n",
    "print(\"average_rougeL_score: \",average_rougeL_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b181593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.38172194361686707\n",
      "BERTScore Recall: 0.26615068316459656\n",
      "BERTScore F1: 0.31311601400375366\n"
     ]
    }
   ],
   "source": [
    "import bert_score\n",
    "\n",
    "model_type = 'facebook/bart-large-cnn'\n",
    "\n",
    "P, R, F1 = bert_score.score(summaries, references, lang='en', model_type=model_type)\n",
    "bertscore_precision = P.mean().item()\n",
    "bertscore_recall = R.mean().item()\n",
    "bertscore_f1 = F1.mean().item()\n",
    "\n",
    "print(\"BERTScore Precision:\", bertscore_precision)\n",
    "print(\"BERTScore Recall:\", bertscore_recall)\n",
    "print(\"BERTScore F1:\", bertscore_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
