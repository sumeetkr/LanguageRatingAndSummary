{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdebe830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587d496a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2004, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"AAAI 2024/kidsInMindSubtitles2004 (2).csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05d7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d5ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1d82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['subtitles'].tolist()\n",
    "labels_regression = df['Language'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bbe8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_subtitle(subtitle, max_tokens):\n",
    "    if len(subtitle) <= max_tokens:\n",
    "        return subtitle\n",
    "    \n",
    "    words = subtitle.split()\n",
    "\n",
    "    shortened_subtitle = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if len(shortened_subtitle) + len(word) + 1 <= max_tokens:\n",
    "            shortened_subtitle += word + \" \"\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    shortened_subtitle = shortened_subtitle.strip()\n",
    "\n",
    "    return shortened_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab5e4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_language_score(subtitle):\n",
    "    # Language model prompt for GPT-3.5\n",
    "    \n",
    "    prompt = f\"Given the subtitle: '{subtitle}', only print the language rating for the given subtitle in the range of 1 to 10 indicating the proportion and context of profane words\"\n",
    "    \n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a language rater.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,  # Set a reasonable value for max_tokens\n",
    "        temperature=0.7,  # Adjust this parameter to control the response randomness\n",
    "    )\n",
    "\n",
    "    # Extract the predicted language score from the model's response\n",
    "    predicted_score = response.choices[0].text.strip()\n",
    "\n",
    "    return predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8364776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_subtitle = shorten_subtitle(texts[1803], 2000)\n",
    "predicted_score = generate_language_score(shortened_subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e8a6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc237cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23c04bd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for text in texts[1943:]:\n",
    "    shortened_subtitle = shorten_subtitle(text, 2000)\n",
    "    predicted_score = generate_language_score(shortened_subtitle)\n",
    "    predicted_scores.append(predicted_score)\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15638f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_scores[0:201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06e1fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_chatgpt_regression=pd.DataFrame({\"Ground_Truth\":labels_regression[1803:],\"Predictions\":predicted_scores[0:201]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd8609b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_chatgpt_regression.to_excel(\"predictions_chatgpt_regression.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "961fab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37e88ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.read_excel(\"predictions_chatgpt_regression.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ecd53e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Ground_Truth</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Numeric Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>and phrases.\\n\\nRating: 0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>used.\\n\\n7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>in the subtitle.\\n\\nRating: 0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>.\\n\\n7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>.\\n\\nLanguage rating: 7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Ground_Truth                    Predictions  Numeric Value\n",
       "0           0             6      and phrases.\\n\\nRating: 0              0\n",
       "1           1             7                     used.\\n\\n7              7\n",
       "2           2             7  in the subtitle.\\n\\nRating: 0              0\n",
       "3           3             9                         .\\n\\n7              7\n",
       "4           4             1        .\\n\\nLanguage rating: 7              7"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d998e336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 18.99502487562189\n",
      "Root Mean Squared Error: 4.3583282202722975\n",
      "Mean Absolute Error: 3.6616915422885574\n",
      "r2:  -1.446125305997552\n",
      "evs:  -1.2500828743370054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "true_labels=labels_regression[1803:]\n",
    "predictions=df_new['Numeric Value'].tolist()\n",
    "mse = mean_squared_error(true_labels, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_labels, predictions)\n",
    "r2 = r2_score(true_labels, predictions)\n",
    "evs = explained_variance_score(true_labels, predictions)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"r2: \",r2)\n",
    "print(\"evs: \",evs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
