{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a2ee12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0639dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7dc0441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2004, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"kidsInMindSubtitles2004 (2).csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3e5caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BartTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edd79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"  \n",
    "class BartForRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BartForRegression, self).__init__()\n",
    "        self.bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.regression_head = torch.nn.Linear(self.bart_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bart_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "        hidden_state = outputs.decoder_hidden_states[-1]\n",
    "        regression_output = self.regression_head(self.dropout(hidden_state[:, 0]))\n",
    "\n",
    "        return regression_output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7dd540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtitlesDataset(Dataset):\n",
    "    def __init__(self, texts, labels_regression, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels_regression = labels_regression\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label_regression = float(self.labels_regression[idx])\n",
    "\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return input_ids, attention_mask, label_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744596b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['subtitles'].tolist()\n",
    "labels_regression = df['Language'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243f0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels_regression, val_labels_regression = train_test_split(texts, labels_regression, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6cfd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a767bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SubtitlesDataset(train_texts, train_labels_regression, tokenizer, max_length=128)\n",
    "val_dataset = SubtitlesDataset(val_texts, val_labels_regression, tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6e1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecbcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2.2908675418875646e-06)\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b07c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.2033, grad_fn=<MseLossBackward0>)\n",
      "tensor(35.1059, grad_fn=<MseLossBackward0>)\n",
      "tensor(27.3035, grad_fn=<MseLossBackward0>)\n",
      "tensor(26.9465, grad_fn=<MseLossBackward0>)\n",
      "tensor(30.7320, grad_fn=<MseLossBackward0>)\n",
      "tensor(22.5247, grad_fn=<MseLossBackward0>)\n",
      "tensor(32.1578, grad_fn=<MseLossBackward0>)\n",
      "tensor(29.5099, grad_fn=<MseLossBackward0>)\n",
      "tensor(25.9635, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.1398, grad_fn=<MseLossBackward0>)\n",
      "tensor(23.1998, grad_fn=<MseLossBackward0>)\n",
      "tensor(21.7848, grad_fn=<MseLossBackward0>)\n",
      "tensor(20.1854, grad_fn=<MseLossBackward0>)\n",
      "tensor(13.4324, grad_fn=<MseLossBackward0>)\n",
      "tensor(16.4642, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.8907, grad_fn=<MseLossBackward0>)\n",
      "tensor(14.5080, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1321, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.3681, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6055, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8660, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0470, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6070, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.3595, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7086, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7384, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6384, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8733, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2696, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7927, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9504, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2616, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.4906, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7493, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.3668, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6988, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1611, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4114, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8386, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5012, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0138, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0651, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8311, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9497, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5862, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1906, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8284, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0660, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0286, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6252, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3683, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0749, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7009, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9952, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.2831, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6043, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2194, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4896, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8370, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4609, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8690, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3873, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8662, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5865, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6713, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8453, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.1722, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0607, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6038, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8076, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8304, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0689, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5035, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9052, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4779, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8626, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2592, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4588, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5585, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8786, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1786, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6235, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7323, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5858, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4753, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.7228, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8405, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5425, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2869, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8517, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4606, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1476, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0287, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2293, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2675, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.5282, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6087, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9175, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.7739, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8130, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1529, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7464, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4520, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.7141, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9694, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4250, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3199, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5372, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3448, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2644, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6604, grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4275, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7086, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1877, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9851, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3588, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3650, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5260, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6104, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8010, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1682, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7077, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9908, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7823, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8859, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8749, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4067, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7798, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4394, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1843, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0875, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9676, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6682, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7092, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7857, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8960, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4859, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4861, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6717, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8530, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3528, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1419, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6140, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1790, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9768, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9827, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8530, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1350, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1460, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1877, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1123, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2877, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9004, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0286, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6485, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6933, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1913, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9367, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0812, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3833, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0507, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4044, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6367, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3870, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6015, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.5594, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2390, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7339, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1529, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5977, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1033, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6280, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7210, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4789, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5303, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5728, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1449, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6808, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4376, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3332, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3798, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6555, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3554, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1993, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8081, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5469, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1462, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6434, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6061, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2391, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5707, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2589, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6089, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9919, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6272, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9042, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1509, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8675, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0628, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1525, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3429, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0915, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9476, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7602, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2339, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4156, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3498, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8428, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.0694, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2146, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2246, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5384, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7349, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9069, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0457, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5061, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9020, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3379, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9954, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0142, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0547, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5673, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7517, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7619, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5244, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4958, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5047, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0364, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5484, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8995, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9472, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5321, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2317, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1190, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0493, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7996, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5188, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7689, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8391, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4868, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0578, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4867, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6980, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4019, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4775, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2944, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4470, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5302, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8289, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5673, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9490, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2201, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1677, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6333, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6621, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3946, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1046, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2417, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8637, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5316, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5418, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5512, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9026, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3776, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5471, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1559, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1240, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7250, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0996, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4442, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7041, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2382, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5300, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8380, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5217, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4109, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0156, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6753, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5756, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9732, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2964, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5888, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7265, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6108, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2189, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6868, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1080, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4034, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5425, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9374, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9943, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9121, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6627, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5819, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2761, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0504, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0208, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5320, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1575, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3975, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6601, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3680, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5048, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8196, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2545, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2994, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4322, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8316, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8096, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0601, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9719, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.7614, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0439, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7360, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0117, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2288, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4019, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5727, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7378, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6744, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2389, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7068, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6635, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4375, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0142, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2071, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8155, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1551, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3029, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4040, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3181, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6782, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8788, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2013, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6850, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0032, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9138, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9484, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2185, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2543, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2730, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9367, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5661, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0656, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9340, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9918, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3340, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5792, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8126, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3751, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3755, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3792, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0169, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7274, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6790, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5154, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8494, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5818, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7100, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8159, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1974, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1629, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3398, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0561, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6700, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6116, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9623, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1375, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2220, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7959, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0363, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1184, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0031, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9925, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3680, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2750, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2802, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4818, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8916, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6884, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4478, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6321, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5971, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2223, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6055, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1369, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6678, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0609, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.5735, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0323, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3188, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9843, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9445, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3818, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6236, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1211, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5163, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0411, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1239, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9930, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4552, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2599, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6696, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5227, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6500, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7640, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6419, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9921, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3475, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8125, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6434, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3356, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7453, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6685, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3217, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9111, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0419, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4468, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2037, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9303, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3298, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1311, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4210, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1197, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6687, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.5816, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7042, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2564, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6718, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0349, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1893, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7958, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0850, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0239, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0612, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8530, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3446, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6490, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4143, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4541, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8429, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8330, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5292, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7426, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8081, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0748, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3819, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8211, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0415, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0772, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1312, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9931, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0631, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9549, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0593, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5691, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6502, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7471, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6607, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9588, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8661, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0993, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5980, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6544, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6774, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6804, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2732, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2024, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8105, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9442, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4947, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7483, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3372, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4471, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8804, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3543, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5928, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3687, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6366, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4841, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5062, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8506, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1041, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8252, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5188, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2882, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1614, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4383, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8108, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9666, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8812, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3190, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6015, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0906, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8485, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8470, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5881, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4611, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2086, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2763, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6492, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6745, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4004, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2055, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7356, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0354, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1877, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9365, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2431, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8579, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7464, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7975, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4753, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2435, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4688, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7100, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3511, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3907, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5089, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7612, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8034, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3044, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0314, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8797, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5581, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6795, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2754, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5829, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9019, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6245, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8614, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9201, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0608, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7738, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6383, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7027, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2952, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5337, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1122, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3234, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6712, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7637, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2769, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9676, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7372, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6812, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0490, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7024, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0306, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2844, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6261, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0221, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7566, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5796, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3886, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7443, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2212, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4035, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2811, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8280, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3155, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6766, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9705, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8702, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3680, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1539, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5490, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3235, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7420, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8632, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9186, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3334, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7476, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9464, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8204, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6936, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3482, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7220, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9746, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6444, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6968, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4051, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9182, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0997, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0965, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1670, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4543, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5522, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2906, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8778, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7981, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9155, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3065, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5931, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7049, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9012, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6278, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7962, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1947, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7765, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3270, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7960, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9740, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8709, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0204, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2333, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0691, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1590, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1123, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5457, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6771, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0459, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5179, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7745, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4315, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2776, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6533, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3719, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0196, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4615, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5168, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6285, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9484, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2107, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3612, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0294, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7257, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8566, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3883, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5631, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2623, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1288, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6672, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7912, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0706, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5124, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7239, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6089, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1698, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0407, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7343, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1733, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6343, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5251, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2113, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4243, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4516, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9391, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1201, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3499, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3425, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5006, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2268, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7912, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5849, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7879, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8517, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1238, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3670, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8142, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3532, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5884, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2592, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9100, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1900, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1973, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4003, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6453, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1402, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8414, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6532, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0670, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8678, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6196, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9867, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4120, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3143, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3644, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0253, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9278, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2196, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9785, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9454, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3752, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4301, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1736, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0995, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3503, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6377, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3153, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3778, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7949, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2407, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2409, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2098, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3862, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1384, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6031, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7154, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5504, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4271, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4179, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8439, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0812, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1611, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3975, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8726, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7367, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9002, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5255, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7352, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9180, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6194, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9629, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8025, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9986, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8264, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4829, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5924, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0173, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0814, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8855, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5255, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7835, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3157, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7678, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9120, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8455, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3121, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3826, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5690, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6206, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8704, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1062, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3291, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4531, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2238, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2692, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9015, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1424, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7895, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0990, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4205, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9584, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7310, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3996, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3601, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5281, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6098, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4252, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2017, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1642, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0602, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0361, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2557, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9669, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2638, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5692, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1994, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3811, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4344, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6885, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6576, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7902, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7723, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8172, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8479, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7223, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8170, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6317, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6743, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3596, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8605, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3234, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4802, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8285, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2931, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2414, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2222, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9740, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2079, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7328, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9654, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4598, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7638, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4351, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2270, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5045, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8557, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5573, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9977, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8973, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6266, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6947, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3200, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0217, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9582, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0904, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8515, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8515, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3081, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6608, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5247, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5161, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8230, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2970, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0637, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4840, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0719, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9639, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0100, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5756, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0180, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4601, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4955, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8197, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6713, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9937, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6878, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4554, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2493, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4363, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9333, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7532, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6236, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0788, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8069, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6337, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7000, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8831, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4501, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0239, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4179, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels_regression = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        regression_output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        labels_regression = labels_regression.float()\n",
    "        regression_output = regression_output.squeeze(-1)\n",
    "        loss = torch.nn.functional.mse_loss(regression_output, labels_regression)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels_regression = batch\n",
    "            regression_output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            regression_output = regression_output.squeeze(-1)\n",
    "            val_loss = torch.nn.functional.mse_loss(regression_output, labels_regression)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9b88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Epoch': range(1, num_epochs + 1),\n",
    "        'Train Loss': train_losses,\n",
    "        'Validation Loss': val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d26215dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a753953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Validation Losses per Epoch saved to: losses_per_epoch2.xlsx\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data)\n",
    "excel_filename = \"losses_per_epoch2.xlsx\"\n",
    "df2.to_excel(excel_filename, index=False)\n",
    "print(\"Train and Validation Losses per Epoch saved to:\", excel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931b1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/BartForRegression.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cf7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/BartForRegression.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f251cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df['subtitles'][1700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ae73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(test_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Get input_ids and attention_mask from the tokenizer output\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d16d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    regression_output = model(input_ids=input_ids, attention_mask=attention_mask).squeeze(-1)\n",
    "\n",
    "predicted_score = regression_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7de49210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Regression Score: 5.3539605140686035\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Regression Score:\", predicted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3de6114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Language'][1700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfccb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel=pd.read_excel(\"../losses_per_epoch.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0d05a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7b5a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Epoch', 'Train Loss', 'Validation Loss'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excel.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602dc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e76fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, 16), df_excel['Train Loss'].tolist(), label='Total Train Loss')\n",
    "plt.plot(range(1, 16), df_excel['Validation Loss'].tolist(), label='Total Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss For BART for Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51fe8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7836e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in df.subtitles:\n",
    "    inputs = tokenizer(sub, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        regression_output = model(input_ids=input_ids, attention_mask=attention_mask).squeeze(-1)\n",
    "        predicted_score = regression_output.item()\n",
    "    predictions.append(predicted_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c693d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions=pd.DataFrame({ 'Language_Score':df['Language'].tolist(),\"Predcited_Score\":predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe64f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Language_Score</th>\n",
       "      <th>Generated_Language_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.210726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8.236761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.133732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.616990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7.753266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Language_Score  Generated_Language_Score\n",
       "0           0               5                  5.210726\n",
       "1           1              10                  8.236761\n",
       "2           2               5                  5.133732\n",
       "3           3               5                  4.616990\n",
       "4           4               9                  7.753266"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833c9ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Language_Score</th>\n",
       "      <th>Generated_Language_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>1803</td>\n",
       "      <td>6</td>\n",
       "      <td>7.661299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>1804</td>\n",
       "      <td>7</td>\n",
       "      <td>4.607816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>1805</td>\n",
       "      <td>7</td>\n",
       "      <td>4.454864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>1806</td>\n",
       "      <td>9</td>\n",
       "      <td>7.200356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>1807</td>\n",
       "      <td>1</td>\n",
       "      <td>5.119037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>7.409991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>6.262559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>6.096935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>6.214080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>2003</td>\n",
       "      <td>6</td>\n",
       "      <td>6.160556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Language_Score  Generated_Language_Score\n",
       "1803        1803               6                  7.661299\n",
       "1804        1804               7                  4.607816\n",
       "1805        1805               7                  4.454864\n",
       "1806        1806               9                  7.200356\n",
       "1807        1807               1                  5.119037\n",
       "...          ...             ...                       ...\n",
       "1999        1999               1                  7.409991\n",
       "2000        2000               1                  6.262559\n",
       "2001        2001               6                  6.096935\n",
       "2002        2002               6                  6.214080\n",
       "2003        2003               6                  6.160556\n",
       "\n",
       "[201 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions[1803:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eed4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2b4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions=pd.read_excel('../predictions_BartForRegression.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38de49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Language_Score</th>\n",
       "      <th>Predcited_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.582466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10.517991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5.170567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5.419154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7.819332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.298275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>5.550373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>2002</td>\n",
       "      <td>6</td>\n",
       "      <td>6.657488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>2003</td>\n",
       "      <td>6</td>\n",
       "      <td>6.153259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2004 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Language_Score  Predcited_Score\n",
       "0              0               5         5.582466\n",
       "1              1              10        10.517991\n",
       "2              2               5         5.170567\n",
       "3              3               5         5.419154\n",
       "4              4               9         7.819332\n",
       "...          ...             ...              ...\n",
       "1999        1999               1         1.017422\n",
       "2000        2000               1         1.298275\n",
       "2001        2001               6         5.550373\n",
       "2002        2002               6         6.657488\n",
       "2003        2003               6         6.153259\n",
       "\n",
       "[2004 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46590e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "true_labels=np.array(df_predictions['Language_Score'].tolist())\n",
    "predicted_scores=np.array(df_predictions['Predcited_Score'].tolist())\n",
    "mse = mean_squared_error(true_labels[1803:], predicted_scores[1803:])\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_labels, predicted_scores)\n",
    "r2 = r2_score(true_labels, predicted_scores)\n",
    "evs = explained_variance_score(true_labels, predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b542806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  1.8038604974228398\n",
      "rmse:  1.3430787383555887\n",
      "mae:  0.5848266576805307\n",
      "r2:  0.8662430136720121\n",
      "evs:  0.8698529514030195\n"
     ]
    }
   ],
   "source": [
    "print(\"mse: \",mse)\n",
    "print(\"rmse: \",rmse)\n",
    "print(\"mae: \",mae)\n",
    "print(\"r2: \",r2)\n",
    "print(\"evs: \",evs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
